---
title: "ICL4.3"
output: html_document
---

```{r}
library(tidyverse)
library(infer)
library(skimr)
```

```{r}
stars <- c(84, 57, 63, 99, 72, 46, 76, 91, rep(NA, 4))
plain <- c(81, 74, 56, 69, 66, 62, 69, 61, 87, 65, 44, 69)
sneetches <- data.frame(stars, plain)
sneetches_tidy <- sneetches %>%
  gather(group, SSI, factor_key = TRUE) %>%
  filter(!is.na(SSI))
sneetches_tidy
```

```{r}
sneetch_plot <- ggplot(sneetches_tidy, aes(x = group, 
                                           y = SSI)) + 
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(position = position_jitter(height = 0, width = 0.2), 
              fill = "lightseagreen", 
              colour = "lightseagreen", 
              alpha = 0.75, 
              size = 4, 
              na.rm=TRUE)
suppressWarnings(print(sneetch_plot))
```


```{r}
mean_diff <- sneetches_tidy %>% 
  specify(SSI ~ group) %>% 
  calculate(stat = "diff in means", 
            order = c("stars", "plain")) 
mean_diff
```


```{r}
set.seed(2018)
sn1 <- sneetches_tidy %>% 
  specify(SSI ~ group) %>% 
  hypothesize(null = "independence") %>% 
  generate(reps = 1, type = "permute")
sn1
```


```{r}
skim_with(numeric = list(hist = NULL))

# original mean
skim(sneetches_tidy)
```

Take a few minutes to confirm for yourself, using skimr for example, that:

The overall N is the same in this resample as in our original sample (20)
  yes

The per-group n's are the same in this resample as in our original sample
  

The overall mean of SSI is the same as in your original sample data
  

The group means of SSI in this resample are different from in your original
sample data (this is what will change for each replicate!)
    

```{r}
# original means by group
sneetches_tidy %>% 
  group_by(group) %>% 
  skim()
```

```{r}
# resample mean
skim(sn1)
```



```{r}
# resampled means by group
sn1 %>% 
  group_by(group) %>% 
  skim()
```

Remember, our observed mean is 6.58. So, this new resampled mean difference is 5.96, calculated as if it didn't matter if the real stars were stars, is much smaller. Now, this was just one possible resample. How many possible ways are there to choose 8 observations from 20?

```{r}
choose(20, 8)
```

This number is not so crazy because we have pretty small sample sizes, but with real data, you'll often find the number of possible permutations is pretty unmanageable. So we make do with an approximation: we will take a large number of resamples, resampling with replacement from the null distribution of (m+nm) possible resamples. Sampling without replacement would be more accurate, but it would require too much time and memory to check the uniqueness of each resample. Long story short: we don't create all possible resamples in a permutation test, which is why this is referred to as a Monte Carlo permutation test.

## Distribution of δ under H0
We can now proceed in a similar way to what we have done previously with bootstrapping by repeating this process many times to create simulated samples, assuming the null hypothesis is true.

```{r}
set.seed(1980)
null_distn <- sneetches_tidy %>% 
  specify(SSI ~ group) %>% 
  hypothesize(null = "independence") %>% 
  generate(reps = 1000, type = "permute") %>% 
  calculate(stat = "diff in means", 
            order = c("stars", "plain"))
```

A null distribution of simulated differences in sample means is created with the specification of stat = "diff in means" for the calculate() step. The null distribution is similar to the bootstrap distribution we saw in Chapter 9, but remember that it consists of statistics generated assuming the null hypothesis is true. Let's plot the permutation distribution, which is the distribution of mean differences across all permutation resamples:

```{r}
null_distn %>% 
  visualize()
```

So here, what you are looking at, is our new null distribution- one that is not based on any distributional assumptions. Rather, this null distribution (the permutation distribution) is based on our sample data, and we ask "in how many permutation resamples did we get a [insert statistic here] as or more extreme than the one we got with our actual sample data?"

So, how many resampled mean differences are as or more extreme than the one we got? The answer to this question is why we calculate the p-value.

### The p-value
Remember that we are interested in seeing where our observed sample mean difference of 6.5833333 falls on this null/randomization distribution. We are interested stars being greater than plain, so "more extreme" corresponds to values in the right tail on the distribution. Let's shade our null distribution to show a visual representation of our p-value:

```{r}
null_distn %>% 
  visualize(obs_stat = mean_diff, direction = "greater")
```

Remember that the observed difference in means was 6.5833333. We have shaded red all values at or above that value. By giving obs_stat = mean_diff, a vertical darker line is also shown at 6.5833333.

At this point, it is important to take a guess as to what the p-value may be. We can see that there are only a few permuted differences as or more large than our observed effect. Lastly, we calculate the p-value directly using infer:

```{r}
mean_diff_number <- mean_diff %>% 
  pull(1)

(pvalue <- null_distn %>%
  get_pvalue(obs_stat = mean_diff, direction = "greater"))
```

```{r}
# same as...
null_distn %>% 
  count(val = stat >= mean_diff_number) 
```

```{r}
159/1000
```


We have around 15.9% of values as or more large than our observed statistic. Assuming we are using a 5% significance level for α, we lack evidence supporting the conclusion that the mean SSI scores are higher in star-bellied compared to plain-bellied sneetches.

## Corresponding confidence interval
One of the great things about the infer pipeline is that going between hypothesis tests and confidence intervals is incredibly simple. To create a null distribution, we ran:

```{r}
null_distn <- sneetches_tidy %>% 
  specify(SSI ~ group) %>% 
  hypothesize(null = "independence") %>% 
  generate(reps = 1000, type = "permute") %>% 
  calculate(stat = "diff in means", 
            order = c("stars", "plain"))
```

To get the corresponding bootstrap distribution with which we can compute a confidence interval, we can just remove or comment out the hypothesize() step since we are no longer assuming the null hypothesis is true when we bootstrap:

```{r}
boot_sneetch_ci <- sneetches_tidy %>% 
  specify(SSI ~ group) %>% 
  #hypothesize(null = "independence") %>% 
  generate(reps = 1000, type = "bootstrap") %>% 
  calculate(stat = "diff in means", 
            order = c("stars", "plain")) %>% 
  get_ci()

boot_sneetch_ci
```

```{r}
#see ci
null_distn %>% 
  visualize(endpoints = boot_sneetch_ci, 
            direction = "between")
```

The question to ask yourself when looking at this plot is: is 0 in my 95% confidence interval? If it is, then a difference of 0 is plausible, and I cannot reject the null hypothesis.

## Assumptions & Caveats
Permutation tests cannot solve all problems: they are valid only when the null hypothesis is 'no association'. Pooling the data to do a two-sample permutation test does require that the two populations (not necessarily samples) have the same distribution when the null hypothesis is true, that is, the mean, spread, and shape are the same. But you should feel fairly confident that, for example, there is not bias present in one sample. Like all methods, it will only work if your samples are representative - always be careful about selection biases! You may also get into dangerous territory if you have sample sizes in your two groups that are pretty uneven (also known as unbalanced), and this is accompanied by group differences in spread. When groups are the same size, the Type I error rate is typically close to the nominal level, otherwise it can be too high or too low.

